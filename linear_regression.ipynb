{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Dq70_ID_jCBq",
        "1HdgNHBHwPuv",
        "wiwwQRvMmynC",
        "Eq2vfcvkbqVr",
        "OfVUbOPUkYY9"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamilest/oxwocs-girls-who-ml-2021/blob/main/linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RpYGnqwYUv2"
      },
      "source": [
        "# Girls Who ML Session 1: Linear regression\n",
        "\n",
        "Authors: Kamilė Stankevičiūtė, Shuyu Lin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wg1F8fGFnJO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn \n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWbYWatzQf5X"
      },
      "source": [
        "#@title Helper functions\n",
        "\n",
        "from sklearn.utils import Bunch\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "dataset = load_iris()\n",
        "sample = [ 57, 122, 118,  53, 117,  81,  70, 142,  84,  16, 103,  82,  66, 31,  83]\n",
        "test_sample = [89, 20, 72, 67, 12]\n",
        "\n",
        "petal_length = np.take(dataset.data[:, 2], sample)\n",
        "petal_width = np.take(dataset.data[:, 3], sample)\n",
        "iris_data = Bunch(data=petal_length, target=petal_width)\n",
        "\n",
        "test_petal_length = np.take(dataset.data[:, 2], test_sample)\n",
        "test_petal_width = np.take(dataset.data[:, 3], test_sample)\n",
        "\n",
        "def plot_iris_dataset(a=None, b=None, model=None, residuals=False, test=False, savefig=False, figname=None):\n",
        "  if test:\n",
        "    plt.scatter(iris_data.data, iris_data.target, alpha=0.2, zorder=0)\n",
        "    plt.scatter(test_petal_length, test_petal_width, c='red', zorder=0)\n",
        "  else:\n",
        "    plt.scatter(iris_data.data, iris_data.target, zorder=0)\n",
        "  \n",
        "  x = np.linspace(0, 8, 100).reshape(-1, 1)\n",
        "  if model:\n",
        "    plt.plot(x, model.predict(x), c='orange', zorder=0)\n",
        "  elif a is not None and b is not None:\n",
        "    plt.plot(x, a * x + b, c='orange', zorder=0)\n",
        "\n",
        "  \n",
        "  if residuals:\n",
        "    if test:\n",
        "      X = test_petal_length.reshape(-1, 1)\n",
        "      y = test_petal_width\n",
        "    else:\n",
        "      X = iris_data.data.reshape(-1, 1)\n",
        "      y = iris_data.target\n",
        "    if model:\n",
        "      y_pred = model.predict(X)\n",
        "    elif a is not None and b is not None:\n",
        "      y_pred = a * X + b\n",
        "    plt.vlines(X, y, y_pred, colors='red', zorder=1)\n",
        "  \n",
        "  plt.xlabel(\"petal length (cm)\")\n",
        "  plt.ylabel(\"petal width (cm)\")\n",
        "  plt.xlim([-0.25, 8.25])\n",
        "  plt.ylim([-0.25, 2.75])\n",
        "\n",
        "  if savefig:\n",
        "    plt.savefig('{}.jpeg'.format(figname if figname else 'figure'), dpi=600)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def plot_iris_dataset_with_line(a, b, residuals=False, test=True, **kwargs):\n",
        "  plot_iris_dataset(a=a, b=b, residuals=residuals, test=test, **kwargs)\n",
        "\n",
        "# These functions are needed for the 3D dataset\n",
        "def predict_house_price(x1, x2, a):\n",
        "    y = a[0]*x1 + a[1]*x2 + a[2]\n",
        "    return y\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_3d_data_samples(data, a=None, plane=False):\n",
        "  x, y, z = data\n",
        "\n",
        "  fig = go.Figure()\n",
        "\n",
        "  fig.add_trace(go.Scatter3d(\n",
        "          x=x, y=y, z=z, mode='markers'))\n",
        "  \n",
        "  if plane:\n",
        "    xmin = np.amin(x)\n",
        "    xmax = np.amax(x)\n",
        "    ymin = np.amin(y)\n",
        "    ymax = np.amax(y)\n",
        "    \n",
        "    x1 = np.linspace(xmin, xmax, 100)\n",
        "    x2 = np.linspace(ymin, ymax, 100)\n",
        "\n",
        "    X1, X2 = np.meshgrid(x1, x2)\n",
        "    Y = predict_house_price(X1, X2, a)\n",
        "\n",
        "    fig.add_trace(go.Surface(\n",
        "        x=x1, y=x2, z=Y))\n",
        "    \n",
        "  fig.update_layout(scene = dict(\n",
        "      xaxis_title=\"Location\",\n",
        "      yaxis_title=\"Size (sqm)\",\n",
        "      zaxis_title=\"Price (k)\"),\n",
        "      width=700, \n",
        "      margin=dict(r=20, b=10, l=10, t=10)\n",
        "  )\n",
        "  fig.show()\n",
        "\n",
        "def plot_3d_data_samples_with_fitted_plane(data, a):\n",
        "  plot_3d_data_samples(data, a, plane=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq70_ID_jCBq"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In the first class we will be learning about one of the simplest (yet still one of the most important) machine learning algorithms called *linear regression*, which is used for *linear models*.\n",
        "\n",
        "## Simple linear model\n",
        "\n",
        "In a linear model, we assume that the *labels* we want to predict can be obtained through a *linear transformation* of the *features*.\n",
        "\n",
        "When we have only one feature, the linear model reduces to the equation of a line, which we are all familiar with:\n",
        "\n",
        "$$y = ax + b$$\n",
        "\n",
        "Here the feature is denoted by $x$, the label by $y$, and $(a, b)$ are the *parameters* of the model.\n",
        "\n",
        "The goal of the *linear regression* method, then, is to obtain the coefficients $(a, b)$ that best model the data (our features and labels). We explore how this is done in detail through an example below.\n",
        "\n",
        "*Aside*. When reading machine learning literature you may encounter alternative terms for features and labels. Features may also be referred to as covariates, predictor variables, or independent variables. Labels are also known as targets, response variables, or dependent variables. They all have the same meaning though!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_l1SNmKjJkB"
      },
      "source": [
        "# The dataset\n",
        "\n",
        "We will be using a subset of the [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) with just 15 points, predicting the petal width from petal length of iris flowers.\n",
        "\n",
        "Here is some code to load the dataset—we extract the petal widths into an array called `petal_width` and petal lengths into `petal_length` (don't worry about the details of the remaining code). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXxdH17IZCDY"
      },
      "source": [
        "from sklearn.utils import Bunch\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "dataset = load_iris()\n",
        "# A list of examples we want to select from the iris dataset for demonstration purposes.\n",
        "sample = [ 57, 122, 118,  53, 117,  81,  70, 142,  84,  16, 103,  82,  66, 31,  83]\n",
        "\n",
        "# np.take selects specific examples from the array representing the full dataset.\n",
        "petal_length = np.take(dataset.data[:, 2], sample)\n",
        "petal_width = np.take(dataset.data[:, 3], sample)\n",
        "iris_data = Bunch(data=petal_length, target=petal_width)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x67WSvQ4pwGN"
      },
      "source": [
        "We can print the values of our feature variable for all data points. Out feature variable is the `petal_length`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwEJyDH0t4VA"
      },
      "source": [
        "petal_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqLgiOv_uD4W"
      },
      "source": [
        "And the corresponding labels, in our case `petal_width`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_Kt-dlYuGDs"
      },
      "source": [
        "petal_width"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEXk5dIwuIc1"
      },
      "source": [
        "While the above format is more convenient for most machine learning tasks, we can also print the data points in the form $(x_i, y_i)$, where $x_i$ is the feature (petal length) and $y_i$ is the label (petal width) for observation $i \\in \\{0, \\dots, N-1\\}$ (where $N$ is the number of observations; note that Python indexes arrays starting from 0). We can access the $i$-th observation like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aTBxBgtvv7G"
      },
      "source": [
        "i = 0\n",
        "petal_length[i], petal_width[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBTEMJasv4no"
      },
      "source": [
        "Or we could print all the data points in this way at the same time:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-fUZSvbuh72"
      },
      "source": [
        "list(zip(petal_length, petal_width))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QhOX7hQupXt"
      },
      "source": [
        "Finally, the most convenient way to explore a dataset is, of course, to visualise it!\n",
        "\n",
        "In this notebook, we can use the magic  `plot_iris_dataset()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBoHhSSFZeC2"
      },
      "source": [
        "plot_iris_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HdgNHBHwPuv"
      },
      "source": [
        "# Fitting a simple linear model\n",
        "\n",
        "Recall that our model is of the form \n",
        "\n",
        "$$y = ax + b$$\n",
        "\n",
        "or, in our case of the iris dataset,\n",
        "\n",
        "$$\\text{petal_width} = a \\cdot \\text{petal_length} + b$$\n",
        "\n",
        "for some unknown $a$ and $b$. Our goal today will be to find the line (parameterised by $a$ and $b$) which fits our dataset the best.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiwwQRvMmynC"
      },
      "source": [
        "## Task 1: experimenting with lines\n",
        "\n",
        "First, let's experiment with some possible values for $a$ and $b$. Try out some lines using another magic plotting function, setting different values for `a` and `b`.\n",
        "\n",
        "*Can you find a line that best matches the data?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7K74g0UxpSd"
      },
      "source": [
        "a = 0.1\n",
        "b = 0\n",
        "plot_iris_dataset_with_line(a, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwWM8J_Y1ZCa"
      },
      "source": [
        "# Quantifying the residuals\n",
        "\n",
        "How could you tell whether one line you tried was better than another? You probably used some notion of \"closeness\" of the line to the data points. For example, the line\n",
        "\n",
        "![](https://raw.githubusercontent.com/kamilest/oxwocs-girls-who-ml-2021/main/images/bad_line.jpeg)\n",
        "\n",
        "probably looks worse than this other line below.\n",
        "\n",
        "![](https://raw.githubusercontent.com/kamilest/oxwocs-girls-who-ml-2021/main/images/better_line.jpeg)\n",
        "\n",
        "Can we quantify this error? The answer is yes, and we can do this by computing the *residuals*. \n",
        "\n",
        "The *residual* is just the *difference* between the value predicted by the model, $\\hat{y}_i$, and the ground truth value $y_i$ for some observation $i$. For the data points in the iris dataset, we can visualise the residuals (the differences between the petal widths predicted by the line and the petal widths in the dataset) as follows:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7sfAc_O5YpC"
      },
      "source": [
        "plot_iris_dataset_with_line(a=0.1, b=0, residuals=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKWRUHuj5la_"
      },
      "source": [
        "where the residuals are shown with the red lines. You can see that the total length of the red lines above is larger than that in the following plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v1bSweA1Xqa"
      },
      "source": [
        "plot_iris_dataset_with_line(a=0.4, b=-0.2, residuals=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoZ3z7XA563X"
      },
      "source": [
        "However, since we do not really care about whether the residual is above or below the line, and because we want  the penalty to scale faster for very large distances, we square the value of the residual:\n",
        "\n",
        "$$ (\\hat{y}_i - y_i)^2$$\n",
        "\n",
        "To account for all the information in the dataset and not just the single point, we compute the average of the squared residuals for all points (assuming there are $N$ points in total, and again accounting for the fact that Python is 0-indexed):\n",
        "\n",
        "$$ \\frac{1}{N} \\sum_{i=0}^{N-1} (\\hat{y}_i - y_i)^2$$\n",
        "\n",
        "which is, in fact, one of the most popular *loss functions*, or *performance measures*, in machine learning called the *mean squared error* (MSE). Our next task will be to compute it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOOps6TxDbwR"
      },
      "source": [
        "## Task 2: computing the MSE\n",
        "\n",
        "Recall that $\\hat{y}_i$ is the value predicted by the model $\\hat{y}_i = a x_i + b$, where $x_i$ in our case is the *petal length*, $y_i$ is the *petal width*. \n",
        "\n",
        "The final expression for the mean squared error is therefore\n",
        "\n",
        "$$ \\frac{1}{N} \\sum_{i=0}^{N-1} (a x_i + b - y_i)^2$$\n",
        "\n",
        "The second task will ask you to write a function to compute the mean squared error of a given line for the iris dataset.\n",
        "\n",
        "*Hint:* With all $x_i$ stored in `x` and all $y_i$ stored in `y`, the $x_i$ and $y_i$ in the dataset can be accessed through `x[i]` and `y[i]` respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e5j7bQi8rKW"
      },
      "source": [
        "def mean_squared_error(a, b, data):\n",
        "  x, y = data\n",
        "\n",
        "  mse = None # TODO: write some code here\n",
        "  \n",
        "  return mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzH6vsO8-DG4"
      },
      "source": [
        "Verify that your answer is correct by checking that, for `data` consisting of `(petal_length, petal_width)`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqN7iMRbMcd8"
      },
      "source": [
        "data = (petal_length, petal_width)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4d7DgvWMgY7"
      },
      "source": [
        "your code returns:\n",
        "* around 1.184 for `mean_squared_error(0.1, 0, data)` and\n",
        "* around 0.04376 for `mean_squared_error(0.4, -0.2, data)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHrqswgDBOVW"
      },
      "source": [
        "mean_squared_error(0.1, 0, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZT8rF3kM9sW"
      },
      "source": [
        "mean_squared_error(0.4, -0.2, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI4HUaaaDrIk"
      },
      "source": [
        "# Computing the analytic solution\n",
        "\n",
        "As we discussed in class, the best coefficients $a$ and $b$ that minimise the mean squared error can be found analytically, using a bit of calculus.\n",
        "\n",
        "In particular, we set the *gradient* of the MSE loss function to 0 in order to obtain the *least squares estimate* for $a$ and $b$. For MSE denoted by $\\mathcal{L}(a, b)$, setting \n",
        "\n",
        "$$ \\dfrac{\\partial \\mathcal L }{\\partial a} := 0 $$\n",
        "\n",
        "and \n",
        "\n",
        "$$ \\dfrac{\\partial \\mathcal L }{\\partial b} := 0 $$\n",
        "\n",
        "we obtain the least squares estimate\n",
        "\n",
        "$$ a = \\dfrac{\\sum_i(x_i - \\bar x)(y_i - \\bar y)}{\\sum_i(x_i - \\bar x)^2}$$\n",
        "&nbsp;\n",
        "$$ b = \\bar y - a \\bar x$$\n",
        "\n",
        "where $\\bar x$ is the mean value of $[x_0, \\dots, x_{N-1}]$ and so on.\n",
        "\n",
        "Task 3 will ask you to use this solution to compute the least squares estimate for the iris dataset directly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03vAlPwlBYWy"
      },
      "source": [
        "## Task 3: computing the least squares estimate\n",
        "\n",
        "Use the above expressions for $a$ and $b$ that minimise the MSE to get the least squares coefficients for the iris dataset.\n",
        "* *Hint*: you might find it helpful to first compute the sum terms separately, e.g. compute `sum_xy`, `sum_x`, `sum_y` and use these variables in the expressions for `a` and `b`.\n",
        "\n",
        "Plot the resulting line using the magic function `plot_iris_dataset_with_line(a, b)` (and the residuals if you like). \n",
        "* How does this line fit the data? \n",
        "\n",
        "Compute the MSE for the coefficients you obtained. \n",
        "* How does it compare to the MSE of the lines you tried to find by hand?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTIygp22GaQS"
      },
      "source": [
        "def iris_least_squares_estimate(data):\n",
        "  x, y = data \n",
        "  a = None\n",
        "  b = None\n",
        "  # TODO: compute the values for the expressions for a and b in the previous\n",
        "  # section.\n",
        "  \n",
        "  return a, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG7W9hNkHy6_"
      },
      "source": [
        "# Plotting the resulting line\n",
        "data = (petal_length, petal_width)\n",
        "a, b = iris_least_squares_estimate(data)\n",
        "plot_iris_dataset_with_line(a=a, b=b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXi3p3RfH_zP"
      },
      "source": [
        "# Computing the MSE of the resulting line\n",
        "mean_squared_error(a, b, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYgFYzKiIEqn"
      },
      "source": [
        "Verify your answers: \n",
        "* The MSE for the least squares estimate should be around 0.0139. \n",
        "* Your new line should look like this: \n",
        "\n",
        "![](https://raw.githubusercontent.com/kamilest/oxwocs-girls-who-ml-2021/main/images/iris_fitted.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVqXRuIC-nOU"
      },
      "source": [
        "# Predicting petal widths for new flowers\n",
        "\n",
        "In the previous sections, we found the coefficients `a` and `b` that best describe the dataset of our 15 observations. However, the main reason we do this estimation is so that we can predict the labels for *new*, unseen observations.\n",
        "\n",
        "To estimate how well our model *generalises* to unseen data, we normally use a *test* dataset. Let's first plot some test values in red, with the original values (our *training dataset*) in lighter shade:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_3C_TIh_sfI"
      },
      "source": [
        "# Get indices for five test points from the full dataset.\n",
        "test_sample = [89, 20, 72, 67, 12]\n",
        "\n",
        "test_petal_length = np.take(dataset.data[:, 2], test_sample)\n",
        "test_petal_width = np.take(dataset.data[:, 3], test_sample)\n",
        "\n",
        "plot_iris_dataset(test=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUoWGykWCRxR"
      },
      "source": [
        "## Task 4a: plotting the estimated line\n",
        "\n",
        "Use one of the magic plotting functions to plot the regression line (which you found the parameters for using the least squares estimate).\n",
        "\n",
        "*Hint:* to include the test points you can simply add the argument `test=True` to the function you decide to use.\n",
        "\n",
        "*How does the least squares line fit the new values?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP1nPEX8Dtzs"
      },
      "source": [
        "# TODO: call a function to plot the regression line for the test dataset."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-E1y6YZCCuw"
      },
      "source": [
        "You may notice that the line does not exactly fit the new data points. We can print the labels predicted by the model and compare them to the ground truth labels, as well as compute the MSE for the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmN616pzGI47"
      },
      "source": [
        "## Task 4b: predicting the labels for test data\n",
        "\n",
        "Predict the labels for the test data points. Most of the code has been filled out for you—you only need to compute the predictions using the `a` and `b` you estimated earlier. \n",
        "\n",
        "*Hint:* Recall that `y_predicted`, or $\\hat{y}_i$, is computed as \n",
        "\n",
        "$$ \\hat{y}_i = ax_i + b$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XxQLpkiGW5d"
      },
      "source": [
        "def predict_petal_widths(a, b):\n",
        "  for (x, y_true) in zip(test_petal_length, test_petal_width):\n",
        "    y_predicted = None # TODO: compute the prediction\n",
        "\n",
        "    print('Ground truth: {}\\tPrediction: {}'.format(y_true, y_predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgU896nqHxy4"
      },
      "source": [
        "predict_petal_widths(a, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jZCRwilRnrV"
      },
      "source": [
        "Verify your code is correct by checking that `predict_petal_widths(a, b)` for your least squares estimate `(a, b)` returns \n",
        "```\n",
        "Ground truth: 1.3\tPrediction: 1.2878410398754319\n",
        "Ground truth: 0.2\tPrediction: 0.5063299708889041\n",
        "Ground truth: 1.5\tPrediction: 1.5936497190440733\n",
        "Ground truth: 1.0\tPrediction: 1.3218197820052808\n",
        "Ground truth: 0.1\tPrediction: 0.404393744499357\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHiweVskIVv5"
      },
      "source": [
        "## Task 4c: computing test MSE\n",
        "\n",
        "We can also compute the test mean squared error. Since the MSE function was conveniently dependent on the `data` argument, we only need to pass the test points instead of the training points to get the test MSE.\n",
        "\n",
        "*Is the test MSE larger than the MSE you computed for the training data?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBP21jttIVKp"
      },
      "source": [
        "data = (test_petal_length, test_petal_width)\n",
        "mean_squared_error() # TODO: pass the relevant arguments here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we-ndM4vNdK6"
      },
      "source": [
        "Verify that your code is correct by checking that your test MSE is now around 0.0598.\n",
        "\n",
        "*Aside.* You may now notice why it is useful to take the *average* of the squared residuals rather than their sum (i.e. compute the *mean* squared error). We have 15 training points and just 5 test points. If we just added up the residuals, we wouldn't be able to compare the errors directly—the training error would likely be larger just because there are more training points. But we can compare the *average* error for datasets of different sizes, observing that, in fact, the error for the test dataset is larger on average.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obKObrELxQJZ"
      },
      "source": [
        "# Higher dimensional input features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNCTTRUEX66-"
      },
      "source": [
        "In this section, you will try to fit a linear regression model to a dataset with\n",
        "higher dimensional input features. We choose to model the housing price as a fucntion of the property's location and its size, using a synthetic dataset that we created. \n",
        "\n",
        "First, try to plot this dataset using the code below. You can rotate the 3D graph to observe the data better. Are the data samples linear? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3oKHZSmXoux"
      },
      "source": [
        "# generate the housing price dataset\n",
        "location = np.array([1, 1, 2, 2, 3, 3, 4, 4, 4, 5])\n",
        "size = np.array([50, 90, 30, 40, 36, 45, 40, 78, 108, 200])\n",
        "price = np.array([300, 500, 300, 350, 450, 500, 400, 600, 800, 400])\n",
        "\n",
        "data_3d = (location, size, price)\n",
        "\n",
        "plot_3d_data_samples(data_3d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LruYrN13uF8z"
      },
      "source": [
        "## Task 5a: Create the big data matrix X\n",
        "\n",
        "Now *can you create the data matrix that stores all the input features?* \n",
        "\n",
        "Recall that \n",
        "\n",
        "$$\\mathbf X = \\begin{bmatrix} \\mathbf{x}_0^\\top \\\\ \\mathbf{x}_1^\\top \\\\ \\vdots \\\\ \\mathbf{x}_{N-1}^\\top \\end{bmatrix}, \\;\\;\\text{where}\\;\\; \\mathbf{x}_i=\\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d\\end{bmatrix}$$\n",
        "\n",
        "*Hint:* for vector and matrix operations, you can use a popular Python scientific computing library called NumPy:\n",
        "1. First, reshape each of the feature arrays (in this case `location` and `size`) into *column* vectors of shape `(N, 1)`.* For the NumPy array `x`, this can be done using `x.reshape()` function, passing your desired shape as the argument. You might also notice that `location` and `size` are NumPy arrays already, so no need to explicitly transform them into the NumPy format, just reshaping.\n",
        "2. Recall that we also needed to append ones to every feature vector (i.e. $\\mathbf x_i = [x_1, \\dots, x_d, 1]^\\top$) to avoid explicitly modelling the bias $b$. Alternatively, we can create another feature consisting of just ones using the `np.ones(shape)` function, where `shape` should be the same as the shape of your other column feature vectors.\n",
        "3. Concatenate all the reshaped feature vectors (`location`, `size`, and the vector of `ones` that you created) into a matrix $\\mathbf X$ using `np.concatenate((reshaped_location, reshaped_size, ones), axis=1)` function.\n",
        "4. Remember that `y` is the *column* vector of labels: $$ \\mathbf y = \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_{N-1}\\end{bmatrix}$$ In our case, the labels are stored in the numpy array `price`. Don't forget, as before, to reshape it into a column!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V2shMMCamBL"
      },
      "source": [
        "N = len(location) # get the number of observations\n",
        "\n",
        "# TODO: create the X matrix \n",
        "X = None\n",
        "\n",
        "# TODO: create the y vector\n",
        "y = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79V430paR9E-"
      },
      "source": [
        "Check your vector `X` now looks like this:\n",
        "\n",
        "```\n",
        "[[  1.  50.   1.]\n",
        " [  1.  90.   1.]\n",
        " ...\n",
        " [  5. 200.   1.]]\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQyUPdlIR6xX"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLSOgEb01jMm"
      },
      "source": [
        "and `y` should look like this:\n",
        "\n",
        "```\n",
        "[[300]\n",
        " [500]\n",
        " ...\n",
        " [400]]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98g6XW1N183T"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq2vfcvkbqVr"
      },
      "source": [
        "### Aside on row and column vectors\n",
        "\n",
        "If at this point all these matrices and transposes seem a bit confusing, it's okay—they often take some time to get used to. But to hopefully make it a bit easier to understand why the observations are transposed to row vectors and why the features need to be transposed into column vectors, it might be useful to take a look at the entire matrix $\\mathbf X$:\n",
        "\n",
        "$$ \\mathbf X = \\begin{bmatrix}\n",
        "x_{1}^{(1)} & x_{2}^{(1)} & \\dots & x_{d}^{(1)} & 1 \\\\\n",
        "x_{1}^{(2)} & x_{2}^{(2)} & \\dots & x_{d}^{(2)} & 1 \\\\\n",
        "\\vdots & \\vdots & \\vdots  & \\vdots & \\vdots \\\\\n",
        "x_{1}^{(N)} & x_{2}^{(N)} & \\dots & x_{d}^{(N)} & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Here I switched the notation a bit (another of the many conventions in ML literature)—in this case $x_i^{(m)}$ means the $i$-th feature of the $m$-th observation in the dataset. Also note that I index the examples starting from 1 again because it is easier to read, but in Python code they will all range from $0$ to $N-1$ for the observations, and from $0$ to $d-1$ for the features.\n",
        "\n",
        "Taking a look at the first row of the matrix,\n",
        "$$[x_{1}^{(1)}, x_{2}^{(1)}, \\dots, x_{d}^{(1)}, 1]$$ \n",
        "\n",
        "you can see that it contains all the $d$ features (as well as the constant term $1$) for the first observation (superscript $\\phantom\\cdot^{(1)}$). But because the vectors are normally represented as columns (and the $i$-th observation $\\mathbf x^{(i)}$ is interpreted as a column vector), to put the column vector $\\mathbf x^{(i)}$ into the matrix $\\mathbf X$ as we just did, we need to transpose it into a row $\\mathbf x^{(i)\\top}$.\n",
        "\n",
        "Similarly, by taking a look at the first column\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "x_1^{(1)} \\\\\n",
        "x_1^{(2)} \\\\\n",
        "\\vdots \\\\\n",
        "x_1^{(N)} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "you can see that it contains the values for the first feature (subscript $\\cdot_1$) for all examples in the dataset (superscripts $\\phantom\\cdot^{(1)}$, ..., $\\phantom\\cdot^{(N)}$). \n",
        "\n",
        "Since our data is grouped by features (arrays `location` and `size`) and not by observations, we construct the big matrix `X` by combining the *columns* representing the different features, rather than the rows representing the different observations. By taking a look at the last column of $\\mathbf X$ you may also see why we needed to create a column vector of `ones`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTYotidnv9_W"
      },
      "source": [
        "## Task 5b: Compute the least squares estimate **a**\n",
        "\n",
        "Now *can you compute the least squares estimate $\\mathbf{a}$?* \n",
        "\n",
        "Recall $$ \\mathbf{a} = (\\mathbf X^\\top \\mathbf X)^{-1} \\mathbf X^\\top \\mathbf y $$\n",
        "\n",
        "*Hints*: For vector operations, use the following functions from the NumPy library:\n",
        "* `np.linalg.inv` for matrix inversion\n",
        "* `np.matmul` for matrix multiplication\n",
        "* `np.transpose` for matrix transpose\n",
        "\n",
        "As always, ask your demonstrator if unsure!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtC4KYsBmWvN"
      },
      "source": [
        "# TODO: calculate the coefficient vector `a`\n",
        "a = None "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG1hufiYSUPK"
      },
      "source": [
        "Your `a` should be the column vector of estimated coefficients:\n",
        "\n",
        "```\n",
        "[[4.26498433e+01]\n",
        " [2.56593753e-01]\n",
        " [3.17917682e+02]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UwtgoumSU92"
      },
      "source": [
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GIYxt75wWNW"
      },
      "source": [
        "Now plot the plane using the estimated coefficient $\\mathbf{a}$. \n",
        "\n",
        "*Is this plane a good fit to our data?*\n",
        "\n",
        "Discuss with your peers and the demonstrators about your insights. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKTqFZfUpJq8"
      },
      "source": [
        "plot_3d_data_samples_with_fitted_plane(data_3d, a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfVUbOPUkYY9"
      },
      "source": [
        "# Bonus: linear regression with `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eJGErNM0HPy"
      },
      "source": [
        "You have now familiarised yourself with the foundations of machine learning and the inner workings of the linear regression model by building it from scratch. Congratulations on making it this far! 🥳\n",
        "\n",
        "While building the model yourself is absolutely the best way to understand it, in practice it is more efficient (and less bug-prone) to use a well-documented machine learning library that can do the work for you.\n",
        "\n",
        "This section will show you how to fit a linear regression model using a popular Python machine learning library called scikit-learn, or `sklearn`. \n",
        "\n",
        "In fact, with the [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class, we can fit a model in just a single line of code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0qY_hxZcdTw"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# X, y is the common convention for representing features and labels.\n",
        "# In our case X is petal_length and y is petal_width.\n",
        "X = petal_length\n",
        "y = petal_width\n",
        "\n",
        "model = LinearRegression().fit(X.reshape(-1, 1), y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayd9uOW9Yjd5"
      },
      "source": [
        "Here, the `model` is an instance of the `LinearRegression` class. After calling `fit` with features `X` and labels `y`, the internal state of the `model` was updated to parameters that best fit the data (i.e. the least squares estimate).\n",
        "\n",
        "We can obtain the best fit parameters as follows. *Do they match the parameters you computed by hand?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEU-AyOuZTTl"
      },
      "source": [
        "a = model.coef_[0]\n",
        "b = model.intercept_\n",
        "print(\"Coefficient: {}, intercept: {}\".format(a, b))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}